sapply(train,class)
#run a linear regression:
lm_basic <- lm(SalePrice~.,data=train)
plot(lm_basic)
summary(lm_basic)
#ridge regression
library(glmnet)
library(glmnetUtils)
lambdas <- 10^seq(3, -2, by = -.1)
#need to turn back to data.frame
train <- as.data.frame(unclass(train))
test <- as.data.frame(unclass(test))
fit <- glmnet(SalePrice~.,data=train, alpha = 0, lambda = lambdas)
summary(fit)
ridge <- cv.glmnet(SalePrice~.,data=train, alpha = 0, lambda = lambdas)
#lasso
fit <- glmnet(SalePrice~.,data=train, alpha = 1, lambda = lambdas)
summary(fit)
lasso <- cv.glmnet(SalePrice~.,data=train, alpha = 1, lambda = lambdas)
#random forest for the kicks:
library(randomForest)
library(caret)
rf <- randomForest(SalePrice~.,data=train)
models <- list(lm_basic,step,ridge,lasso,rf)
models <- list(lm_basic,#step,
ridge,lasso,rf)
getBestModel <- function(models,tests = test) {
mse <- unlist(lapply(models,function(x) {
predicted <- predict(x,tests)
return( sum((predicted - tests$SalePrice)^2) )
}))
return(models[[which.min(mse)]])
}
bestModel <- getBestModel(models)
predict(models[[1]],test)
predict(models[[2]],test)
summary(models[[2]])
ridge <- cv.glmnet(SalePrice~.,data=train, alpha = 0, lambda = lambdas)
?cv.glmnet
ridge <- glmnetUtils::cv.glmnet(SalePrice~.,data=train, alpha = 0, lambda = lambdas)
#lasso
fit <- glmnet(SalePrice~.,data=train, alpha = 1, lambda = lambdas)
lasso <- glmnetUtild::cv.glmnet(SalePrice~.,data=train, alpha = 1, lambda = lambdas)
lasso <- glmnetUtils::cv.glmnet(SalePrice~.,data=train, alpha = 1, lambda = lambdas)
#random forest for the kicks:
library(randomForest)
library(caret)
rf <- randomForest(SalePrice~.,data=train)
models <- list(lm_basic,#step,
ridge,lasso,rf)
predict(models[[1]],test)
predict(models[[2]],test)
?predict
predict(fit,data=test)
class(test)
class(fit)
predict(fit)
predict(fit,data=test)
predict(ridge,test)
class(test)
predict(ridge,test)
sparsed_test_data <- Matrix(data=0,
nrow=nrow(test),
ncol=ncol(train),
dimnames=list(rownames(test),
colnames(train)),
sparse = T)
for(i in colnames(test_data)){
sparsed_test_data[, i] <- test_data[, i]
}
for(i in colnames(test_data)){
sparsed_test_data[, i] <- test[, i]
}
for(i in colnames(test)){
sparsed_test_data[, i] <- test[, i]
}
library(Matrix)
for(i in colnames(test)){
sparsed_test_data[, i] <- test[, i]
}
summary(train)
rm(list=ls())
#get the data
train <- read.csv("https://jagterberg.github.io/assets/masters2020/train.csv")
#alternative to dplyr: data.table
library(data.table)
train <- data.table(train)
na_vals <- sapply(train,function(x) {
return(!any(is.na(x)))
})
#the train[..na_vals] removes the ones with na_vals
train <- train[,..na_vals]
#-- cleaned code --
names(train)
summary(train)
#this removes Id
train[,Id:= NULL]
#split to train and test
set.seed(4442)
training <- sample(c(1:nrow(train)),floor(2/3*nrow(train)))
testing <- setdiff(c(1:nrow(train)),training)
train[, `:=` (Utilities = NULL
,Condition2 = NULL
,RoofStyle = NULL
,RoofMatl = NULL
,ExterCond = NULL
,HeatingQC  = NULL
,Exterior1st = NULL
,Heating=NULL
)]
test <- train[testing]
train <- train[training]
#convert to dataframe
train <- as.data.frame(unclass(train),stringsAsFactors = T)
test <- as.data.frame(unclass(test),stringsAsFactors = T)
#run a linear regression:
lm_basic <- lm(SalePrice~.,data=train)
summary(lm_basic)
plot(lm_basic)
#------------------------------- ML Concepts--------------------------------
#stepwise regression:
library(MASS)
step <- stepAIC(lm_basic)
summary(step)
plot(step)
lambdas <- 10^seq(3, -2, by = -.1)
#need to turn back to data.frame
train <- as.data.frame(unclass(train),stringsAsFactors = T)
test <- as.data.frame(unclass(test),stringsAsFactors = T)
fit <- glmnet(SalePrice~.,data=train, alpha = 0, lambda = lambdas)
summary(fit)
ridge <- glmnetUtils::cv.glmnet(SalePrice~.,data=train, alpha = 0, lambda = lambdas)
#lasso
fit <- glmnet(SalePrice~.,data=train, alpha = 1, lambda = lambdas)
summary(fit)
lasso <- glmnetUtils::cv.glmnet(SalePrice~.,data=train, alpha = 1, lambda = lambdas)
#random forest for the kicks:
library(randomForest)
library(caret)
rf <- randomForest(SalePrice~.,data=train)
models <- list(lm_basic,#step,
ridge,lasso,rf)
getBestModel <- function(models,tests = test) {
mse <- unlist(lapply(models,function(x) {
predicted <- predict(x,tests)
return( sum((predicted - tests$SalePrice)^2) )
}))
return(models[[which.min(mse)]])
}
bestModel <- getBestModel(models)
models <- list(lm_basic,#step,
#ridge,lasso,
rf)
getBestModel <- function(models,tests = test) {
mse <- unlist(lapply(models,function(x) {
predicted <- predict(x,tests)
return( sum((predicted - tests$SalePrice)^2) )
}))
return(models[[which.min(mse)]])
}
bestModel <- getBestModel(models)
sum(names(test) %in% names(train))
View(train)
rm(list=ls())
#get the data
train <- read.csv("https://jagterberg.github.io/assets/masters2020/train.csv")
#alternative to dplyr: data.table
library(data.table)
train <- data.table(train)
na_vals <- sapply(train,function(x) {
return(!any(is.na(x)))
})
#the train[..na_vals] removes the ones with na_vals
train <- train[,..na_vals]
#-- cleaned code --
names(train)
summary(train)
#this removes Id
train[,Id:= NULL]
#split to train and test
set.seed(4442)
training <- sample(c(1:nrow(train)),floor(2/3*nrow(train)))
testing <- setdiff(c(1:nrow(train)),training)
train[, `:=` (Utilities = NULL
,Condition2 = NULL
,RoofStyle = NULL
,RoofMatl = NULL
,ExterCond = NULL
,HeatingQC  = NULL
,Exterior1st = NULL
,Heating=NULL
)]
test <- train[testing]
train <- train[training]
#convert to dataframe
train <- as.data.frame(unclass(train),stringsAsFactors = T)
test <- as.data.frame(unclass(test),stringsAsFactors = T)
test <- rbind(xtrain[1, ] , xtest)
test <- test[-1,]
#run a linear regression:
lm_basic <- lm(SalePrice~.,data=train)
step <- stepAIC(lm_basic)
lambdas <- 10^seq(3, -2, by = -.1)
#need to turn back to data.frame
train <- as.data.frame(unclass(train),stringsAsFactors = T)
test <- as.data.frame(unclass(test),stringsAsFactors = T)
test <- rbind(xtrain[1, ] , xtest)
test <- rbind(train[1, ] , test)
test <- test[-1,]
fit <- glmnet(SalePrice~.,data=train, alpha = 0, lambda = lambdas)
summary(fit)
ridge <- glmnetUtils::cv.glmnet(SalePrice~.,data=train, alpha = 0, lambda = lambdas)
#lasso
fit <- glmnet(SalePrice~.,data=train, alpha = 1, lambda = lambdas)
summary(fit)
lasso <- glmnetUtils::cv.glmnet(SalePrice~.,data=train, alpha = 1, lambda = lambdas)
#random forest for the kicks:
library(randomForest)
library(caret)
rf <- randomForest(SalePrice~.,data=train)
models <- list(lm_basic,#step,
#ridge,lasso,
rf)
getBestModel <- function(models,tests = test) {
mse <- unlist(lapply(models,function(x) {
predicted <- predict(x,tests)
return( sum((predicted - tests$SalePrice)^2) )
}))
return(models[[which.min(mse)]])
}
bestModel <- getBestModel(models)
models <- list(lm_basic,step,
#ridge,lasso,
rf)
getBestModel <- function(models,tests = test) {
mse <- unlist(lapply(models,function(x) {
predicted <- predict(x,tests)
return( sum((predicted - tests$SalePrice)^2) )
}))
return(models[[which.min(mse)]])
}
bestModel <- getBestModel(models)
models <- list(lm_basic,step,
ridge,lasso,
rf)
getBestModel <- function(models,tests = test) {
mse <- unlist(lapply(models,function(x) {
predicted <- predict(x,tests)
return( sum((predicted - tests$SalePrice)^2) )
}))
return(models[[which.min(mse)]])
}
bestModel <- getBestModel(models)
summary(bestModel)
bestModel
#-------------dimensionality reduction--------------
rm(list = ls())
dat <- iris
#pca:
numeric_vars <- sapply(dat,is.numeric)
new_dat <- prcomp(dat[,numeric_vars])
summary(new_dat)
dat <- cbind(dat,new_dat$x)
#---------------------classification-----------------
dat$setosa_ind <- factor(ifelse(dat$Species == 'setosa',1,0))
dat[,'Species'] <- NULL
summary(dat)
View(dat)
#split data into train and test: (no CV here)
set.seed(1234)
train <- sample(c(1:nrow(dat)),floor(2/3*nrow(dat)))
test <- setdiff(c(1:nrow(dat)),train)
dat_train <- dat[train,]
dat_test <- dat[test,]
logistic <- glm(setosa_ind~.,data=dat_train,family = binomial(link = 'logit'))
summary(logistic)
if(!require(randomForest)) {
install.packages('randomForest')
library(randomForest)
}
rf <- randomForest(setosa_ind~.,data=dat_train)
rf
get_error <- function(model,test = dat_test,var = 'setosa_ind') {
predicted <- predict(model,test)
if("glm" %in% class(model)) {
predicted <- ifelse(predicted > .5,1,0)
}
error <- sum(predicted != test[,var])
error
}
get_error(rf)
get_error(logistic)
logistic2 <- glm(setosa_ind~PC1 + PC2, data =dat_train,family = binomial)
summary(logistic2)
summary(logistic)
get_error(logistic2)
rf_new <- randomForest(setosa_ind~PC1 + PC2, data =dat)
get_error(rf_new)
library(MASS)
attach(cars)
detach(cars)
attach(Cars93)
plot(MPG.city,Price, pch = as.numeric(Type))
plot(MPG.city,Price, pch = as.numeric(Type))
summary(type)
summary(Type)
legend("topright",
legend=c(levels(Type)),
pch=c(NA,1))
levels(Type)
plot(MPG.city,Price, pch = as.numeric(Type))
legend("topright",
legend=c(levels(Type)),
pch=c(NA,1,2,3,4))
plot(MPG.city,Price, pch = as.numeric(Type))
legend("midright",
legend=c(levels(Type)),
pch=c(NA,1,2,3,4))
plot(MPG.city,Price, pch = as.numeric(Type))
legend("right",
legend=c(levels(Type)),
pch=c(NA,1,2,3,4))
plot(MPG.city,Price, pch = as.numeric(Type))
legend("right",
legend=c(levels(Type)),
pch=c(1:6))
plot(MPG.city,Price, pch = as.numeric(Type))
legend("topright",
legend=c(levels(Type)),
pch=c(1:6))
attach(nym.2002)
hist(time)
names(nym.2002)
par()
?boxplot
boxplot(reaction.time~control)
boxplot(reaction.time$time~reaction.time$control)
exhibit_clt <- function(no_means = 1000,sample_size = 10) {
sample_means<- c()
for(i in 1:no_means){
sample_means[i] <- mean(rexp(sample_size))
}
plot(density(sample_means))
}
exhibit_clt(sample_size = 10)
exhibit_clt <- function(no_means = 1000,sample_size = 10,plot=TRUE) {
sample_means<- c()
for(i in 1:no_means){
sample_means[i] <- mean(rexp(sample_size))
}
if(plot) {
plot(density(sample_means))
} else {
return(sample_means)
}
}
test <- exhibit_clt(sample_size= 10,plot=FALSE)
qqnorm(test)
qqline(test)
test <- exhibit_clt(sample_size= 1000,plot=FALSE)
qqnorm(test)
qqline(test)
?dnorm
?dpooiss
?dpoiss
?dpois
dnewdist <- function(x){
vals <- c(0,1,2)
pmf <- c(.3,.2,.5)
if(x==0 | x==1 | x==2) {
return(pmf[x==vals])
}
else {
return(0)
}
}
dnewdist(0)
pnewdist <- function(x){
vals <- c(0,1,2)
prob <- ifelse(x<0, 0, ifelse(x<1,.3,ifelse(x<2,.5,1)))
return(prob)
}
pnewdist <- function(x){
vals <- c(0,1,2)
prob <- ifelse(x<0, 0, ifelse(x<1,.3,ifelse(x<2,.5,1)))
return(prob)
}
pnewdist(1)
pnewdist(.9)
rnewdist <- function(n){
vals <- runif(n)
vec <- ifelse(vals<.3, 0, ifelse(vals<.5,1,2))
return(vec)
}
rnewdist <- function(n){
vals <- runif(n)
vec <- ifelse(vals<.3, 0, ifelse(vals<.5,1,2))
return(vec)
}
means=c(); std_devs=c()
for(i in 1:10000){
vec = rnewdist(5)
means[i] = mean(vec); std_devs[i] = sd(vec)
}
hist(means, freq=F); lines(density(means))
hist(std_devs, freq =F); lines(density(std_devs))
make_hist <- function(n=10,mean=TRUE) {
means=c(); std_devs=c()
for(i in 1:10000){
vec = rnewdist(n)
means[i] = mean(vec); std_devs[i] = sd(vec)
}
if(mean) {
hist(means, freq=F); lines(density(means))
} else {
hist(std_devs, freq =F); lines(density(std_devs))
}
}
make_hist(10)
make_hist(5)
make_hist(10)
make_hist(100)
make_hist(1000)
make_hist(1000)
library(UsingR)
x = rnorm(100)
y = x + 3 + rnorm(100, sd=.25)
res = lm(y~x)
summary(res)
plot(res)
hist(x)
plot(x,y)
summary(res)
x = rnorm(100)
y = x + 3 + rnorm(100, sd=10)
res = lm(y~x)
summary(res)
x = rnorm(100)
y = x + 3 + rnorm(100, sd=.25)
res = lm(y~x)
summary(res)
plot(res)
attach(babies)
summary(babies)
mult_reg_no_factor = lm(
wt ~ gestation + age + gestation*age,
subset = gestation < 350 & age < 99
)
summary(mult_reg_no_factor)
sqrt(.175)
summary(smoke)
summary(as.factor(smoke))
mult_reg_factor = lm(
wt ~ gestation+age+gestation*age+factor(smoke),
subset = gestation < 350 & age < 99 & smoke < 9
)
summary(mult_reg_factor)
summary(factor(smoke))
plot(mult_reg_no_factor)
plot(mult_reg_factor)
detach(babies)
.6*(1-.6)
sqrt(.6*(1-.6))
sqrt(.6*(1-.6)/42)
rm(list=ls())
#get the data
train <- read.csv("https://jagterberg.github.io/assets/masters2020/train.csv")
View(train)
View(train)
#alternative to dplyr: data.table
library(data.table)
train <- data.table(train)
?apply
?sapply
na_vals <- sapply(train,function(x) {
return(!any(is.na(x)))
})
na_vals
#the train[..na_vals] removes the ones with na_vals
train <- train[,..na_vals]
#-- cleaned code --
names(train)
summary(train)
#this removes Id
train[,Id:= NULL]
#split to train and test
set.seed(4442)
2/3*1460
training <- sample(c(1:nrow(train)),floor(2/3*nrow(train)))
testing <- setdiff(c(1:nrow(train)),training)
train[, `:=` (Utilities = NULL
,Condition2 = NULL
,RoofStyle = NULL
,RoofMatl = NULL
,ExterCond = NULL
,HeatingQC  = NULL
,Exterior1st = NULL
,Heating=NULL
)]
test <- train[testing]
train <- train[training]
973+487
#convert to dataframe
train <- as.data.frame(unclass(train),stringsAsFactors = T)
test <- as.data.frame(unclass(test),stringsAsFactors = T)
test <- rbind(xtrain[1, ] , xtest)
#run a linear regression:
lm_basic <- lm(SalePrice~.,data=train)
summary(lm_basic)
predict(lm_basic,data=test)
sum( (predict(lm_basic,data=test) - test$SalePrice)^2)
sum( (predict(lm_basic,data=test) - test$SalePrice)^2)/nrow(test)
