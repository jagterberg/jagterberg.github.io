n <- 100
n
n <- 100
mu <- 0
sigma <- 1
set.seed(124) #set for reproducibility
x <- rnorm(n = n, mean = mu, sd = sigma)
summary(x)
hist(x = x)
x <- rnorm(n = n, mean = mu, sd = sigma)
summary(x)
set.seed(124) #set for reproducibility
x <- rnorm(n = n, mean = mu, sd = sigma)
summary(x)
alpha <- 0.05
mu_LB <- mean(x) - qnorm(alpha/2, lower.tail = FALSE) * sigma / sqrt(n)
?qnorm
mu_UB <-mean(x) + qnorm(alpha/2, lower.tail = FALSE) * sigma / sqrt(n)
cat('C.I. for mu:', mu_LB, mu_UB)
runif(100)
qnorm(alpha/2, lower.tail = FALSE)
qnorm(alpha/2, lower.tail = TRUE)
paste0("hello"," world")
paste0("hello","world")
test_hypothesis_known_variance <- function(mu_0 = .1,dat = NULL,alpha = .05, sigma = 1) {
#error handling:
if (is.null(dat)) {
error('data not provided')
}
toReturn <- list()
#list access:
toReturn[[1]] <- paste0('H0: mu_0 = ',mu_0)
n <- length(dat)
test_stat <- (mean(dat) - mu_0) / (sigma / sqrt(n))
if (abs(test_stat) > qnorm(alpha/2, lower.tail = FALSE)){
toReturn[[2]] <- 'Yes'
} else {
toReturn[[2]] <- 'No'
}
toReturn[[3]] <- 2 * pnorm(abs(test_stat), lower.tail = FALSE)
names(toReturn) <- c('Null Hypothesis'
,paste0("Do We Reject Null at alpha = ",alpha,"?")
, "P-Value")
return(toReturn)
}
View(test_hypothesis_known_variance)
result <- test_hypothesis_known_variance(mu_0=.3,dat = x)
result
summary(x)
result <- test_hypothesis_known_variance(mu_0=.001,dat = x)
result
result$`Null Hypothesis`
result$`P-Value`
rm(list = ls())
n <- 100
mu <- 0
sigma <- 1
set.seed(2) #set for reproducibility
x <- rnorm(n = n, mean = mu, sd = sigma)
summary(x)
alpha <- 0.05
s2 <- sd(x)
mu_LB <- mean(x) - qt(alpha/2, df = n-1, lower.tail = FALSE) * sqrt(s2 / n)
mu_UB <-mean(x) + qt(alpha/2, df = n-1, lower.tail = FALSE) * sqrt(s2 / n)
cat('C.I. for mu:', mu_LB, mu_UB)
sigma2_LB <- (n-1) * s2 / qchisq(alpha/2, df = n-1, lower.tail = FALSE)
sigma2_UB <- (n-1) * s2 / qchisq(alpha/2, df = n-1, lower.tail = TRUE)
cat('C.I. for sigma2:', sigma2_LB, sigma2_UB)
test_hypothesis_unknown_variance <- function(dat, mu_0 = .1,alpha = .05) {
#error handling:
if (is.null(dat)) {
error('data not provided')
}
toReturn <- list()
toReturn[[1]] <- paste0('H0: mu_0 = ',mu_0)
s2 <- sd(dat)
n <- length(dat)
test_stat <- (mean(x) - mu_0) / (s2 / sqrt(n))
if (abs(test_stat) > qt(alpha/2, df = n-1, lower.tail = FALSE)){
toReturn[[2]] <- 'Yes'
}else {
toReturn[[2]] <- 'No'
}
toReturn[[3]] <- 2 * pt(abs(test_stat), df = n-1, lower.tail = FALSE)
names(toReturn) <- c('Null Hypothesis'
,paste0("Do We Reject Null at alpha = ",alpha,"?")
, "P-Value")
return(toReturn)
}
result <- test_hypothesis_unknown_variance(dat = x, mu_0 = .3)
result
result <- test_hypothesis_unknown_variance(dat = x, mu_0 = .0001)
result
mu_X <- 1
mu_Y <- 1
sigma_X <- 1
sigma_Y <- 2
n_X <- 1000
n_Y <- 1000
data_X <- rnorm(n_X, mean = mu_X, sd = sigma_X)
data_Y <- rnorm(n_Y, mean = mu_Y, sd = sigma_Y)
hist(data_X, breaks = 20, freq = FALSE)
hist(data_Y, breaks = 20, freq = FALSE)
x_bar <- mean(data_X)
y_bar <- mean(data_Y)
cat("point estimation:", x_bar - y_bar)
LCB <- x_bar - y_bar + qnorm(0.025) * sqrt(sigma_X^2 / n_X + sigma_Y^2 / n_Y)
1000/20
?hist
hist(data_Y, breaks = 50, freq = FALSE)
hist(data_Y, breaks = 100, freq = FALSE)
hist(data_Y, breaks = 1000, freq = FALSE)
hist(data_Y, breaks = 20, freq = FALSE)
hist(data_X, breaks = 20, freq = FALSE)
?hist
max(data_X)
min(data_X)
(max(data_X) - min(dat_X))/20
(max(data_X) - min(data_X))/20
hist(data_X)
hist(data_X, breaks = 20, freq = FALSE)
hist(data_X,breaks=40)
print("hello world")
cat("hello world")
?cat
print(paste0("hello", " world"))
cat(paste0("hello", " world"))
LCB <- x_bar - y_bar + qnorm(0.025) * sqrt(sigma_X^2 / n_X + sigma_Y^2 / n_Y)
UCB <- x_bar - y_bar + qnorm(0.975) * sqrt(sigma_X^2 / n_X + sigma_Y^2 / n_Y)
cat("interval estimation", LCB, UCB)
s_X <- sd(data_X)
s_Y <- sd(data_Y)
LCB <- x_bar - y_bar + qt(0.025,n_X + n_Y - 2) * sqrt(s_X^2 / n_X + s_Y^2 / n_Y)
UCB <- x_bar - y_bar + qt(0.975,n_X + n_Y - 2) * sqrt(s_X^2 / n_X + s_Y^2 / n_Y)
cat("interval estimation", LCB, UCB)
qt(0.025,n_X + n_Y - 2)
qnorm(0.025)
rm(list = ls())
x <- rnorm(1000000,mean = .05,sd = 2)
nreps <- 1000
s <- rep(0,nreps)
size <- 100
for (i in 1:nreps) {
s[i] <- mean(sample(x,size,replace = FALSE))
}
hist(s,breaks = 200,freq = FALSE)
xfit = seq(min(s),max(s),.01)
yfit <- dnorm(xfit,.05,2/10)
lines(xfit,yfit, col = 'purple', lwd = 2)
abline(v=.05,col = 'black',lwd = 4)
hist(s,breaks = 100,freq = FALSE)
xfit = seq(min(s),max(s),.01)
yfit <- dnorm(xfit,.05,2/10)
lines(xfit,yfit, col = 'purple', lwd = 2)
s <- rep(0,nreps)
size <- 100
for (i in 1:nreps) {
s[i] <- (99/4)*var(sample(x,size,replace = TRUE))
}
hist(s,breaks = 200,freq = FALSE)
xfit = seq(min(s),max(s),.01)
yfit <- dchisq(xfit,99)
lines(xfit,yfit, col = 'purple', lwd = 2)
hist(s,breaks = 50,freq = FALSE)
xfit = seq(min(s),max(s),.01)
yfit <- dchisq(xfit,99)
lines(xfit,yfit, col = 'purple', lwd = 2)
rm(list = ls())
library(MASS)
install.packages("MASS")
install.packages("MASS")
library(MASS)
data(cars)
plot(cars$speed, cars$dist)
View(cars)
?lm
speed_dist_reg <- lm(cars$dist ~ cars$speed)
abline(speed_dist_reg)
summary(speed_dist_reg)
speed_dist_aov <- aov(speed_dist_reg)
summary(speed_dist_aov)
x <- cars$speed
y <- cars$dist
n <- length(x)
Sxy <- sum(x*y) - sum(x) * sum(y) / n
Sxx <- sum(x^2) - sum(x)^2 / n
Syy <- sum(y^2) - sum(y)^2 / n
beta_hat <- Sxy / Sxx
alpha_hat <- mean(y) - beta_hat * mean(x)
alpha_hat
beta_hat
speed_dist_reg$coefficients
rm(list = ls())
getwd()
train <- read.csv("https://jagterberg.github.io/assets/masters2020/train.csv")
View(train)
library(data.table)
train <- data.table(train)
class(train)
na_vals <- sapply(train,function(x) {
return(!any(is.na(x)))
})
na_vals
train <- train[,..na_vals]
names(train)
summary(train)
train[,Id:= NULL]
set.seed(4443)
training <- sample(c(1:nrow(train)),floor(2/3*nrow(train)))
testing <- setdiff(c(1:nrow(train)),training)
train[, `:=` (Utilities = NULL
,Condition2 = NULL
,RoofStyle = NULL
,RoofMatl = NULL
,ExterCond = NULL
,HeatingQC  = NULL
)]
test <- train[testing]
train <- train[training]
train <- as.data.frame(unclass(train))
test <- as.data.frame(unclass(test))
lm_basic <- lm(SalePrice~.,data=train)
plot(lm_basic)
summary(lm_basic)
train <- read.csv("https://jagterberg.github.io/assets/masters2020/train.csv")
library(data.table)
train <- data.table(train)
na_vals <- sapply(train,function(x) {
return(!any(is.na(x)))
})
train <- train[,..na_vals]
names(train)
summary(train)
train[,Id:= NULL]
set.seed(4443)
training <- sample(c(1:nrow(train)),floor(2/3*nrow(train)))
testing <- setdiff(c(1:nrow(train)),training)
train[, `:=` (Utilities = NULL
,Condition2 = NULL
,RoofStyle = NULL
,RoofMatl = NULL
,ExterCond = NULL
,HeatingQC  = NULL
)]
test <- train[testing]
train <- train[training]
train <- as.data.frame(unclass(train))
test <- as.data.frame(unclass(test))
lm_basic <- lm(SalePrice~.,data=train)
plot(lm_basic)
names(train)
summary(lm_basic)
summary(train$Exterior2nd)
library(MASS)
step <- stepAIC(lm_basic)
summary(step)
plot(step)
AIC(lm_basic)
AIC(step)
?AIC
library(glmnet)
library(glmnetUtils)
lambdas <- 10^seq(3, -2, by = -.1)
lambdas
train <- as.data.frame(unclass(train))
test <- as.data.frame(unclass(test))
?glm
fit <- glmnet(SalePrice~.,data=train, alpha = 0, lambda = lambdas)
summary(fit)
?glmnet
ridge <- cv.glmnet(SalePrice~.,data=train, alpha = 0, lambda = lambdas)
fit <- glmnet(SalePrice~.,data=train, alpha = 1, lambda = lambdas)
summary(fit)
lasso <- cv.glmnet(SalePrice~.,data=train, alpha = 1, lambda = lambdas)
library(randomForest)
library(caret)
rf <- randomForest(SalePrice~.,data=train)
?tuneRF
?randomForest
names(train)[!names(train)%in%"SalePrice"]
res <- tuneRF(train[,names(train)[!names(train)%in%"SalePrice"]],train$SalePrice)
mtry_opt <- res[,"mtry"][which.min(res[,"OOBError"])]
mtry_opt
rf <- randomForest(SalePrice~.,data=train,mtry = mtry_opt,ntree=1000)
models <- list(lm_basic,step,ridge,lasso,rf)
?lapply
getBestModel <- function(models,tests = test) {
mse <- unlist(lapply(models,function(x) {
predicted <- predict(x,tests)
return( sum((predicted - tests$SalePrice)^2) )
}))
return(models[[which.min(mse)]])
}
bestModel <- getBestModel(models)
summary(bestModel)
bestModel
summary(train)
summary(lm_basic)
plot(lm_basic)
rm(list = ls())
rm(list = ls())
dat <- iris
dat
View(dat)
numeric_vars <- sapply(dat,is.numeric)
numeric_vars
View(dat)
new_dat <- prcomp(dat[,numeric_vars])
summary(new_dat)
dat <- cbind(dat,new_dat$x)
View(dat)
summary(dat)
View(dat)
x <- c(1,1,2,3)
x + x
rm(x)
ifelse(dat$Species == 'setosa',1,0)
class(ifelse(dat$Species == 'setosa',1,0))
dat$setosa_ind <- factor(ifelse(dat$Species == 'setosa',1,0))
dat[,'Species'] <- NULL
summary(dat)
View(dat)
set.seed(1234)
train <- sample(c(1:nrow(dat)),floor(2/3*nrow(dat)))
test <- setdiff(c(1:nrow(dat)),train)
dat_train <- dat[train,]
dat_test <- dat[test,]
View(dat_test)
View(dat_train)
logistic <- glm(setosa_ind~.,data=dat_train,family = binomial(link = 'logit'))
summary(logistic)
if(!require(randomForest)) {
install.packages('randomForest')
library(randomForest)
}
rf <- randomForest(setosa_ind~.,data=dat_train)
rf
if(!require(tree)) {
install.packages('tree')
library(tree)
}
get_error <- function(model,test = dat_test,var = 'setosa_ind') {
predicted <- predict(model,test)
if("glm" %in% class(model)) {
predicted <- ifelse(predicted > .5,1,0)
}
error <- sum(predicted != test[,var])
error
}
get_error(rf)
get_error(logistic)
logistic2 <- glm(setosa_ind~PC1 + PC2, data =dat,family = binomial)
summary(logistic2)
summary(logistic)
get_error(logistic2)
rf_new <- randomForest(setosa_ind~PC1 + PC2, data =dat)
get_error(rf_new)
logistic2 <- glm(setosa_ind~PC1 + PC2, data =dat_train,family = binomial)
summary(logistic2)
summary(logistic)
get_error(logistic2)
